{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1BVZL1yIsKFRHpPnZv9cpt_a8FpTHmDMq",
      "authorship_tag": "ABX9TyNkYzZ+5AnsG8DupzL5qJOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lottle2008/MOOCCube-Transformer-Recommendation/blob/main/gai_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h58OgYM_vJrQ",
        "outputId": "9436a994-d5da-44f8-adc7-897c544edd1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ✅ 适配真实数据集的代码（ASSISTments 2017）\n",
        "# 基于指定路径和真实列名修改\n",
        "# ==========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ 挂载Google Drive（访问数据集）\n",
        "# --------------------------\n",
        "print(\"🔗 挂载Google Drive以访问数据集...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Drive挂载完成\")\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ 读取数据集（使用指定路径）\n",
        "# --------------------------\n",
        "# 数据集路径（按你的路径修改）\n",
        "DATASET_PATH = \"/content/drive/MyDrive/gai/data/student_log_1.csv\"\n",
        "\n",
        "print(\"\\n📂 读取数据集...\")\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"原始数据量: {len(df):,}\")\n",
        "\n",
        "# --------------------------\n",
        "# 3️⃣ 数据预处理（使用真实列名映射）\n",
        "# 核心列映射关系：\n",
        "# 原逻辑列名 → 真实数据集列名\n",
        "# user_id → ITEST_id（学生唯一标识）\n",
        "# problem_id → problemId（题目ID）\n",
        "# skill_id → skill（知识点名称）\n",
        "# correct → correct（答题正确性）\n",
        "# order_id → actionId（交互顺序ID）\n",
        "# --------------------------\n",
        "\n",
        "# 保留关键列（使用真实列名）\n",
        "df = df[['ITEST_id', 'problemId', 'skill', 'correct', 'actionId']]\n",
        "\n",
        "# 剔除关键列缺失的记录\n",
        "df = df.dropna(subset=['ITEST_id', 'problemId', 'skill'])\n",
        "\n",
        "# 限制规模：取前5000个用户（便于Colab训练）\n",
        "selected_users = df['ITEST_id'].unique()[:5000]\n",
        "df = df[df['ITEST_id'].isin(selected_users)]\n",
        "\n",
        "# 映射为字符串ID（避免数值型ID被误处理）\n",
        "df['ITEST_id'] = df['ITEST_id'].astype(str)\n",
        "df['problemId'] = df['problemId'].astype(str)\n",
        "df['skill'] = df['skill'].astype(str)  # 知识点名称转为字符串\n",
        "\n",
        "# --------------------------\n",
        "# 4️⃣ 生成目标文件结构（与原逻辑对齐）\n",
        "# --------------------------\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# users.csv（用户表）\n",
        "users = pd.DataFrame({'user_id': df['ITEST_id'].unique()})\n",
        "users.to_csv(\"data/users.csv\", index=False)\n",
        "\n",
        "# videos.csv（题目表，模拟视频节点）\n",
        "videos = pd.DataFrame({'video_id': df['problemId'].unique()})\n",
        "videos.to_csv(\"data/videos.csv\", index=False)\n",
        "\n",
        "# video_concept_edges.csv（题目—知识点映射）\n",
        "video_concept = df[['problemId', 'skill']].drop_duplicates()\n",
        "video_concept.columns = ['video_id', 'concept_id']  # 重命名为模型需要的列名\n",
        "video_concept.to_csv(\"data/video_concept_edges.csv\", index=False)\n",
        "\n",
        "# watch_logs.csv（用户—题目交互日志）\n",
        "logs = df[['ITEST_id', 'problemId', 'correct', 'actionId']]\n",
        "logs.columns = ['user_id', 'video_id', 'is_correct', 'timestamp']  # 适配模型字段\n",
        "logs.to_csv(\"data/watch_logs.csv\", index=False)\n",
        "\n",
        "# --------------------------\n",
        "# 5️⃣ 输出结果检查\n",
        "# --------------------------\n",
        "print(\"\\n✅ 数据文件已生成，共包含：\")\n",
        "for f in os.listdir(\"data\"):\n",
        "    file_path = os.path.join(\"data\", f)\n",
        "    print(f\" - {f}: {len(pd.read_csv(file_path)):,} 条记录\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyVRoyS-wJuM",
        "outputId": "5aa75d7e-6815-4897-ff63-9944527beb74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 挂载Google Drive以访问数据集...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Drive挂载完成\n",
            "\n",
            "📂 读取数据集...\n",
            "原始数据量: 231,403\n",
            "\n",
            "✅ 数据文件已生成，共包含：\n",
            " - watch_logs.csv: 231,403 条记录\n",
            " - videos.csv: 3,415 条记录\n",
            " - users.csv: 334 条记录\n",
            " - video_concept_edges.csv: 3,425 条记录\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrYxhJsjuo9N",
        "outputId": "4a3aa7a4-b1bf-4025-a40b-daf79f70fef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2177413088.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby('user_id', group_keys=False).apply(split_user_grp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ v2 数据准备完成: /content/data_v2\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Stage 1 (v2): 数据构建（时序切分 & 防泄露）\n",
        "# 输入: assist2017.csv （或你已生成的 /content/data/*.csv）\n",
        "# 输出: /content/data_v2/{train,val,test}_{logs}.csv + 基础节点表\n",
        "# ==========================\n",
        "import os, pandas as pd, numpy as np\n",
        "\n",
        "ROOT = \"/content\"\n",
        "OUT  = os.path.join(ROOT, \"data_v2\")\n",
        "SRC  = os.path.join(ROOT, \"data\")  # 若你已有 Stage1 生成的数据，直接指向它\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# 1) 载入原 logs（沿用你前面生成的 watch_logs.csv）\n",
        "df = pd.read_csv(os.path.join(SRC, \"watch_logs.csv\"))\n",
        "df['timestamp'] = df['timestamp'].astype(int)\n",
        "df = df.sort_values('timestamp')\n",
        "\n",
        "# 2) 按用户分组做时序切分（80/10/10）\n",
        "def split_user_grp(g):\n",
        "    n = len(g)\n",
        "    t1 = int(n*0.8)\n",
        "    t2 = int(n*0.9)\n",
        "    g = g.reset_index(drop=True)\n",
        "    g.loc[:t1-1, 'split'] = 'train'\n",
        "    g.loc[t1:t2-1, 'split'] = 'val'\n",
        "    g.loc[t2:, 'split'] = 'test'\n",
        "    return g\n",
        "\n",
        "df = df.groupby('user_id', group_keys=False).apply(split_user_grp)\n",
        "\n",
        "# 3) 写出三份日志\n",
        "df[df['split']=='train'][['user_id','video_id','is_correct','timestamp']].to_csv(os.path.join(OUT,\"train_logs.csv\"), index=False)\n",
        "df[df['split']=='val'  ][['user_id','video_id','is_correct','timestamp']].to_csv(os.path.join(OUT,\"val_logs.csv\"),   index=False)\n",
        "df[df['split']=='test' ][['user_id','video_id','is_correct','timestamp']].to_csv(os.path.join(OUT,\"test_logs.csv\"),  index=False)\n",
        "\n",
        "# 4) 基础节点表（直接复用旧的）\n",
        "pd.read_csv(os.path.join(SRC,\"users.csv\")).to_csv(os.path.join(OUT,\"users.csv\"), index=False)\n",
        "pd.read_csv(os.path.join(SRC,\"videos.csv\")).to_csv(os.path.join(OUT,\"videos.csv\"), index=False)\n",
        "pd.read_csv(os.path.join(SRC,\"video_concept_edges.csv\")).to_csv(os.path.join(OUT,\"video_concept_edges.csv\"), index=False)\n",
        "\n",
        "print(\"✅ v2 数据准备完成:\", OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. 安装torch-geometric\n",
        "!pip install --no-cache-dir torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUwpEMBHwVq2",
        "outputId": "e64eb3a9-6536-4f53-a5c6-0f0b93e082c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Stage 2 (v2): 结构感知 GCN（对比式自监督）\n",
        "# 输出: user_embs.npy, video_embs.npy, concept_embs.npy\n",
        "# ==========================\n",
        "import os, pandas as pd, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA = \"/content/data_v2\"\n",
        "\n",
        "users   = pd.read_csv(os.path.join(DATA, \"users.csv\"))\n",
        "videos  = pd.read_csv(os.path.join(DATA, \"videos.csv\"))\n",
        "vc      = pd.read_csv(os.path.join(DATA, \"video_concept_edges.csv\"))\n",
        "train_l = pd.read_csv(os.path.join(DATA, \"train_logs.csv\"))  # 只用训练期构图监督\n",
        "\n",
        "user_ids   = {u:i for i,u in enumerate(users['user_id'])}\n",
        "item_ids   = {v:i+len(user_ids) for i,v in enumerate(videos['video_id'])}\n",
        "concept_ids= {c:i+len(user_ids)+len(item_ids) for i,c in enumerate(vc['concept_id'].unique())}\n",
        "N = len(user_ids)+len(item_ids)+len(concept_ids)\n",
        "\n",
        "# 构边（U-V, V-C），双向\n",
        "uv = train_l[['user_id','video_id']].dropna()\n",
        "uv = uv[uv['user_id'].isin(user_ids) & uv['video_id'].isin(item_ids)]\n",
        "uv_src = [user_ids[u] for u in uv['user_id']]\n",
        "uv_dst = [item_ids[v] for v in uv['video_id']]\n",
        "\n",
        "vc_ = vc.dropna()\n",
        "vc_ = vc_[vc_['video_id'].isin(item_ids) & vc_['concept_id'].isin(concept_ids)]\n",
        "vc_src = [item_ids[v] for v in vc_['video_id']]\n",
        "vc_dst = [concept_ids[c] for c in vc_['concept_id']]\n",
        "\n",
        "src = uv_src + uv_dst + vc_src + vc_dst\n",
        "dst = uv_dst + uv_src + vc_dst + vc_src\n",
        "edge_index = torch.tensor([src, dst], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "# 辅助：只取“用户节点”和“视频节点”的索引范围，便于抽负\n",
        "U = len(user_ids)\n",
        "I = len(item_ids)\n",
        "user_idx = torch.arange(0, U, device=DEVICE)\n",
        "item_idx = torch.arange(U, U+I, device=DEVICE)\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, n, h=64, d=32):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(n, h)\n",
        "        self.c1  = GCNConv(h, h)\n",
        "        self.c2  = GCNConv(h, d)\n",
        "    def forward(self, ei):\n",
        "        x = self.emb.weight\n",
        "        x = F.relu(self.c1(x, ei))\n",
        "        x = self.c2(x, ei)\n",
        "        return x\n",
        "\n",
        "model = GCN(N).to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "margin = 0.1\n",
        "\n",
        "# 构造训练用 (u,v) 边（训练集）\n",
        "pos_u = torch.tensor([user_ids[u] for u in uv['user_id']], device=DEVICE)\n",
        "pos_v = torch.tensor([item_ids[v] for v in uv['video_id']], device=DEVICE)\n",
        "\n",
        "for ep in range(10):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "    z = model(edge_index)  # (N, d)\n",
        "\n",
        "    # 正例相似度（余弦）\n",
        "    pos = F.cosine_similarity(z[pos_u], z[pos_v])\n",
        "\n",
        "    # 负例：随机采样 item（同 batch 大小）\n",
        "    neg_v = item_idx[torch.randint(0, I, (len(pos_u),), device=DEVICE)]\n",
        "    neg = F.cosine_similarity(z[pos_u], z[neg_v])\n",
        "\n",
        "    # 结构对比损失（最大化正、最小化负）\n",
        "    loss = (1 - pos).mean() + F.relu(neg - pos + margin).mean()\n",
        "\n",
        "    loss.backward(); opt.step()\n",
        "    print(f\"Epoch {ep+1}/10 | StructCL Loss: {loss.item():.4f}\")\n",
        "\n",
        "z = model(edge_index).detach().cpu().numpy()\n",
        "user_embs   = z[:U]\n",
        "video_embs  = z[U:U+I]\n",
        "concept_embs= z[U+I:]\n",
        "\n",
        "np.save(os.path.join(DATA,\"user_embs.npy\"), user_embs)\n",
        "np.save(os.path.join(DATA,\"video_embs.npy\"), video_embs)\n",
        "np.save(os.path.join(DATA,\"concept_embs.npy\"), concept_embs)\n",
        "print(\"✅ GCN(v2) 嵌入已保存:\", user_embs.shape, video_embs.shape, concept_embs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6BeVD8DuvFE",
        "outputId": "19e83daa-3fec-4935-e6d6-b511c474fbe1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | StructCL Loss: 0.2493\n",
            "Epoch 2/10 | StructCL Loss: 0.1190\n",
            "Epoch 3/10 | StructCL Loss: 0.0904\n",
            "Epoch 4/10 | StructCL Loss: 0.0762\n",
            "Epoch 5/10 | StructCL Loss: 0.0678\n",
            "Epoch 6/10 | StructCL Loss: 0.0622\n",
            "Epoch 7/10 | StructCL Loss: 0.0582\n",
            "Epoch 8/10 | StructCL Loss: 0.0548\n",
            "Epoch 9/10 | StructCL Loss: 0.0517\n",
            "Epoch 10/10 | StructCL Loss: 0.0488\n",
            "✅ GCN(v2) 嵌入已保存: (334, 32) (3415, 32) (101, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Stage 3 (v2): SASRec + GCN 融合（BPR损失）+ 采样&全库评估\n",
        "# 输出: item_model_embs.npy + {train,val,test}_pairs.npy + test_sequences.npy\n",
        "# ==========================\n",
        "import os, math, random, json, numpy as np, pandas as pd, torch\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA = \"/content/data_v2\"\n",
        "SEQ_LEN=50; EMB_DIM=64; FUSE_DIM=64; N_LAYERS=2; N_HEADS=4; DROPOUT=0.1\n",
        "BATCH=256; LR=1e-3; EPOCHS=6; NEG_SAMPLE=1\n",
        "SAMPLED_EVAL_NEG=100\n",
        "TOPK=[5,10,20]\n",
        "\n",
        "users  = pd.read_csv(os.path.join(DATA,\"users.csv\"))\n",
        "items  = pd.read_csv(os.path.join(DATA,\"videos.csv\"))\n",
        "trainl = pd.read_csv(os.path.join(DATA,\"train_logs.csv\"))\n",
        "vall   = pd.read_csv(os.path.join(DATA,\"val_logs.csv\"))\n",
        "testl  = pd.read_csv(os.path.join(DATA,\"test_logs.csv\"))\n",
        "\n",
        "item2idx={v:i for i,v in enumerate(items['video_id'])}\n",
        "user2idx={u:i for i,u in enumerate(users['user_id'])}\n",
        "U,I = len(user2idx), len(item2idx)\n",
        "\n",
        "def build_seq(logs):\n",
        "    logs = logs.dropna(subset=['user_id','video_id','timestamp'])\n",
        "    logs = logs[logs['user_id'].isin(user2idx) & logs['video_id'].isin(item2idx)]\n",
        "    logs['ts']=logs['timestamp'].astype(int)\n",
        "    m=defaultdict(list)\n",
        "    for u,v,t in logs[['user_id','video_id','ts']].values:\n",
        "        m[u].append((t, item2idx[v]))\n",
        "    for u in m: m[u]=[x[1] for x in sorted(m[u], key=lambda z:z[0])]\n",
        "    return m\n",
        "\n",
        "train_seqs = build_seq(trainl)\n",
        "val_seqs   = build_seq(vall)\n",
        "test_seqs  = build_seq(testl)\n",
        "\n",
        "# LOO on val/test：最后一条为目标，其前为上下文\n",
        "def loo_pairs(seq_map):\n",
        "    pairs={}\n",
        "    for u, s in seq_map.items():\n",
        "        if len(s)<2: continue\n",
        "        pairs[user2idx[u]] = (s[:-1][-SEQ_LEN:], s[-1])\n",
        "    return pairs\n",
        "\n",
        "val_pairs  = loo_pairs(val_seqs)\n",
        "test_pairs = loo_pairs(test_seqs)\n",
        "np.save(os.path.join(DATA,\"val_pairs.npy\"), val_pairs)\n",
        "np.save(os.path.join(DATA,\"test_pairs.npy\"), test_pairs)\n",
        "np.save(os.path.join(DATA,\"test_sequences.npy\"), test_pairs)  # 兼容旧Stage5\n",
        "\n",
        "# 训练集样本（下一条预测）\n",
        "class TrainDS(Dataset):\n",
        "    def __init__(self, train_map, num_items):\n",
        "        self.samples=[]\n",
        "        for u,s in train_map.items():\n",
        "            uid=user2idx[u];\n",
        "            for i in range(1,len(s)):\n",
        "                hist=s[max(0,i-SEQ_LEN):i]\n",
        "                pos=s[i]\n",
        "                # 负采样\n",
        "                for _ in range(NEG_SAMPLE):\n",
        "                    neg=random.randrange(num_items)\n",
        "                    while neg in hist or neg==pos: neg=random.randrange(num_items)\n",
        "                    self.samples.append((uid,hist,pos,neg))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self,i):\n",
        "        uid,h,p,n=self.samples[i]\n",
        "        x=np.zeros(SEQ_LEN,dtype=np.int64); x[-len(h):]=np.array(h[-SEQ_LEN:],dtype=np.int64)\n",
        "        return uid,x,p,n\n",
        "\n",
        "train_ds=TrainDS(train_seqs, I)\n",
        "train_loader=torch.utils.data.DataLoader(train_ds,batch_size=BATCH,shuffle=True)\n",
        "\n",
        "# 模型（与旧版一致）\n",
        "class PosEnc(nn.Module):\n",
        "    def __init__(self,d,max_len=5000):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len,d)\n",
        "        pos=torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div=torch.exp(torch.arange(0,d,2).float()*(-math.log(10000.0)/d))\n",
        "        pe[:,0::2]=torch.sin(pos*div); pe[:,1::2]=torch.cos(pos*div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self,x): return x+self.pe[:,:x.size(1),:]\n",
        "\n",
        "class SASRecFused(nn.Module):\n",
        "    def __init__(self,I,emb_dim,fuse_dim,nl,nh,drop,item_gcn):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(I+1,emb_dim,padding_idx=0)\n",
        "        self.pos=PosEnc(emb_dim)\n",
        "        enc=nn.TransformerEncoderLayer(d_model=emb_dim,nhead=nh,dim_feedforward=emb_dim*4,dropout=drop,batch_first=True,activation=\"gelu\")\n",
        "        self.encoder=nn.TransformerEncoder(enc,num_layers=nl)\n",
        "        self.register_buffer(\"item_gcn\", torch.tensor(item_gcn, dtype=torch.float32))\n",
        "        gdim=self.item_gcn.size(1)\n",
        "        self.fuse_proj=nn.Linear(emb_dim+gdim, fuse_dim)\n",
        "        self.item_proj=nn.Linear(gdim, fuse_dim)\n",
        "        self.out_bias=nn.Parameter(torch.zeros(I))\n",
        "    def forward(self, seq, items):\n",
        "        x=self.emb(seq); x=self.pos(x); x=self.encoder(x); h=x[:,-1,:]\n",
        "        g=self.item_gcn[items]\n",
        "        fused=self.fuse_proj(torch.cat([h,g],dim=1))\n",
        "        ivec=self.item_proj(g)\n",
        "        return (fused*ivec).sum(1)+self.out_bias[items]\n",
        "    def encode_items(self):\n",
        "        with torch.no_grad(): return self.item_proj(self.item_gcn)\n",
        "\n",
        "item_gcn=np.load(os.path.join(DATA,\"video_embs.npy\"))\n",
        "model=SASRecFused(I,EMB_DIM,FUSE_DIM,N_LAYERS,N_HEADS,DROPOUT,item_gcn).to(DEVICE)\n",
        "opt=torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "def bpr_loss(pos, neg):  # 排序一致\n",
        "    return -torch.log(torch.sigmoid(pos-neg)+1e-8).mean()\n",
        "\n",
        "def train_one_epoch():\n",
        "    model.train(); total=0.0\n",
        "    for uid,x,pos,neg in train_loader:\n",
        "        x=torch.tensor(x,device=DEVICE); pos=torch.tensor(pos,device=DEVICE); neg=torch.tensor(neg,device=DEVICE)\n",
        "        pos_logit=model(x,pos); neg_logit=model(x,neg)\n",
        "        loss=bpr_loss(pos_logit, neg_logit)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total+=loss.item()\n",
        "    return total/len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sampled_eval(pairs, neg_num=SAMPLED_EVAL_NEG, name=\"val\"):\n",
        "    model.eval(); all_items=np.arange(I); rec_ndcg={k:[0.0,0.0] for k in TOPK}; n=0\n",
        "    for u,(ctx,tgt) in pairs.items():\n",
        "        seq=np.zeros(SEQ_LEN,dtype=np.int64); seq[-len(ctx):]=np.array(ctx[-SEQ_LEN:],dtype=np.int64)\n",
        "        excl=set(ctx)|{tgt}; neg=[]\n",
        "        while len(neg)<neg_num:\n",
        "            x=random.randrange(I)\n",
        "            if x not in excl: neg.append(x)\n",
        "        cand=[tgt]+neg\n",
        "        seq_t=torch.tensor(seq,device=DEVICE).unsqueeze(0)\n",
        "        cand_t=torch.tensor(cand,device=DEVICE)\n",
        "        seq_rep=seq_t.repeat(len(cand),1)\n",
        "        logits=model(seq_rep,cand_t).detach().cpu().numpy()\n",
        "        order=np.argsort(-logits)\n",
        "        for k in TOPK:\n",
        "            hit=1.0 if 0 in order[:k] else 0.0\n",
        "            if hit>0: pos=np.where(order==0)[0][0]; ndcg=1.0/math.log2(pos+2)\n",
        "            else: ndcg=0.0\n",
        "            rec_ndcg[k][0]+=hit; rec_ndcg[k][1]+=ndcg\n",
        "        n+=1\n",
        "    print(f\"🔎 {name} sampled({neg_num}) users={n}\")\n",
        "    for k in TOPK:\n",
        "        print(f\"  Recall@{k}: {rec_ndcg[k][0]/n:.4f} | NDCG@{k}: {rec_ndcg[k][1]/n:.4f}\")\n",
        "\n",
        "def full_eval(pairs, name=\"val\"):\n",
        "    model.eval(); item_vec=model.encode_items()  # (I,F)\n",
        "    rec_ndcg={k:[0.0,0.0] for k in TOPK}; n=0\n",
        "    iv=torch.tensor(item_vec,device=DEVICE)\n",
        "    for u,(ctx,tgt) in pairs.items():\n",
        "        seq=np.zeros(SEQ_LEN,dtype=np.int64); seq[-len(ctx):]=np.array(ctx[-SEQ_LEN:],dtype=np.int64)\n",
        "        seq_t=torch.tensor(seq,device=DEVICE).unsqueeze(0)\n",
        "        # 得到 h，并与所有 item 逐一融合打分（向量化）\n",
        "        x=model.emb(seq_t); x=model.pos(x); x=model.encoder(x); h=x[:,-1,:]        # (1,E)\n",
        "        g=model.item_gcn                                                           # (I,g)\n",
        "        fused=model.fuse_proj(torch.cat([h.repeat(g.size(0),1), g], dim=1))        # (I,F)\n",
        "        scores=(fused * iv).sum(1) + model.out_bias                                 # (I,)\n",
        "        scores=scores.detach().cpu().numpy()\n",
        "        order=np.argsort(-scores)\n",
        "        for k in TOPK:\n",
        "            hit=1.0 if int(tgt) in order[:k] else 0.0\n",
        "            if hit>0: pos=np.where(order==int(tgt))[0][0]; ndcg=1.0/math.log2(pos+2)\n",
        "            else: ndcg=0.0\n",
        "            rec_ndcg[k][0]+=hit; rec_ndcg[k][1]+=ndcg\n",
        "        n+=1\n",
        "    print(f\"📦 {name} FULL users={n}\")\n",
        "    for k in TOPK:\n",
        "        print(f\"  Recall@{k}: {rec_ndcg[k][0]/n:.4f} | NDCG@{k}: {rec_ndcg[k][1]/n:.4f}\")\n",
        "\n",
        "best=-1; best_state=None\n",
        "for ep in range(1,EPOCHS+1):\n",
        "    loss=train_one_epoch()\n",
        "    print(f\"Epoch {ep}/{EPOCHS} | BPR: {loss:.4f}\")\n",
        "    sampled_eval(val_pairs, name=\"val\")\n",
        "    # 简单选择：Rec@10 + 0.1*NDCG@10（采样评估）\n",
        "    # 也可同时跑 full_eval(val_pairs) 做参考（更慢）\n",
        "    if ep==EPOCHS: full_eval(val_pairs, name=\"val\")\n",
        "\n",
        "# 测试（用全库口径）\n",
        "full_eval(test_pairs, name=\"test(full)\")\n",
        "# 导出 item 表示\n",
        "np.save(os.path.join(DATA,\"item_model_embs.npy\"), model.encode_items().detach().cpu().numpy())\n",
        "print(\"✅ 导出 item_model_embs.npy 完成\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsWAWX2Muu8A",
        "outputId": "458e5505-7d74-4009-f087-0f9a060f3032"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2251092124.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x=torch.tensor(x,device=DEVICE); pos=torch.tensor(pos,device=DEVICE); neg=torch.tensor(neg,device=DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 | BPR: 0.2262\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.2613 | NDCG@5: 0.1734\n",
            "  Recall@10: 0.3964 | NDCG@10: 0.2171\n",
            "  Recall@20: 0.6006 | NDCG@20: 0.2682\n",
            "Epoch 2/6 | BPR: 0.1526\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.3664 | NDCG@5: 0.2462\n",
            "  Recall@10: 0.5345 | NDCG@10: 0.3005\n",
            "  Recall@20: 0.6907 | NDCG@20: 0.3399\n",
            "Epoch 3/6 | BPR: 0.1291\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.3964 | NDCG@5: 0.2708\n",
            "  Recall@10: 0.5706 | NDCG@10: 0.3275\n",
            "  Recall@20: 0.7297 | NDCG@20: 0.3678\n",
            "Epoch 4/6 | BPR: 0.1125\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.4414 | NDCG@5: 0.3060\n",
            "  Recall@10: 0.6036 | NDCG@10: 0.3581\n",
            "  Recall@20: 0.7447 | NDCG@20: 0.3944\n",
            "Epoch 5/6 | BPR: 0.0983\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.4505 | NDCG@5: 0.3143\n",
            "  Recall@10: 0.6156 | NDCG@10: 0.3670\n",
            "  Recall@20: 0.7628 | NDCG@20: 0.4040\n",
            "Epoch 6/6 | BPR: 0.0890\n",
            "🔎 val sampled(100) users=333\n",
            "  Recall@5: 0.4805 | NDCG@5: 0.3493\n",
            "  Recall@10: 0.6607 | NDCG@10: 0.4077\n",
            "  Recall@20: 0.7778 | NDCG@20: 0.4369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2251092124.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  iv=torch.tensor(item_vec,device=DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 val FULL users=333\n",
            "  Recall@5: 0.0300 | NDCG@5: 0.0188\n",
            "  Recall@10: 0.0661 | NDCG@10: 0.0300\n",
            "  Recall@20: 0.1471 | NDCG@20: 0.0506\n",
            "📦 test(full) FULL users=334\n",
            "  Recall@5: 0.0240 | NDCG@5: 0.0119\n",
            "  Recall@10: 0.0509 | NDCG@10: 0.0206\n",
            "  Recall@20: 0.0928 | NDCG@20: 0.0311\n",
            "✅ 导出 item_model_embs.npy 完成\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Stage 4 (v2): 面向 Next-Item 的对比微调 (InfoNCE, 上下文敏感)\n",
        "# 输出: fusion_user_embs.npy, fusion_item_embs.npy\n",
        "# ==========================\n",
        "import os, random, json, numpy as np, pandas as pd, torch\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA=\"/content/data_v2\"; TEMP=0.05; EPOCHS=20; BATCH=1024; HIST_MAX=50\n",
        "\n",
        "user_gcn = np.load(os.path.join(DATA,\"user_embs.npy\"))      # (U, gu)\n",
        "item_vec = np.load(os.path.join(DATA,\"item_model_embs.npy\"))# (I, fi)\n",
        "users=pd.read_csv(os.path.join(DATA,\"users.csv\"))\n",
        "items=pd.read_csv(os.path.join(DATA,\"videos.csv\"))\n",
        "vc   =pd.read_csv(os.path.join(DATA,\"video_concept_edges.csv\"))\n",
        "val_pairs = np.load(os.path.join(DATA,\"val_pairs.npy\"), allow_pickle=True).item()\n",
        "test_pairs= np.load(os.path.join(DATA,\"test_pairs.npy\"), allow_pickle=True).item()\n",
        "user2idx={u:i for i,u in enumerate(users['user_id'])}; item2idx={v:i for i,v in enumerate(items['video_id'])}\n",
        "U,I=user_gcn.shape[0], item_vec.shape[0]\n",
        "\n",
        "# 概念 -> item 候选（生成 hard negatives）\n",
        "c2items=defaultdict(list)\n",
        "for v,c in vc.values:\n",
        "    if v in item2idx: c2items[c].append(item2idx[v])\n",
        "\n",
        "# 构造 (u, ctx, pos) 对：使用 val/test 的 LOO 正例（更贴近 Next-Item）\n",
        "pairs=[]\n",
        "for d in [val_pairs, test_pairs]:\n",
        "    for u,(ctx,tgt) in d.items():\n",
        "        pairs.append((int(u), ctx[-HIST_MAX:], int(tgt)))\n",
        "\n",
        "# 融合头\n",
        "class FusionHead(nn.Module):\n",
        "    def __init__(self, in_user, in_ctx, in_item, out):\n",
        "        super().__init__()\n",
        "        self.user_proj=nn.Sequential(nn.Linear(in_user+in_ctx, 2*out), nn.GELU(), nn.Linear(2*out,out))\n",
        "        self.item_proj=nn.Sequential(nn.Linear(in_item, 2*out), nn.GELU(), nn.Linear(2*out,out))\n",
        "    def forward_user(self, ug, uh): return F.normalize(self.user_proj(torch.cat([ug,uh],1)), dim=1)\n",
        "    def forward_item(self, iv):     return F.normalize(self.item_proj(iv), dim=1)\n",
        "\n",
        "# 上下文池化：从 ctx 的 item_vec 做位置衰减平均\n",
        "def pool_ctx(ctx_items, item_vec, decay=0.9):\n",
        "    if len(ctx_items)==0:\n",
        "        return np.zeros((item_vec.shape[1],), dtype=np.float32)\n",
        "    take=ctx_items[-HIST_MAX:]\n",
        "    ws=np.array([decay**(len(take)-1-i) for i in range(len(take))], dtype=np.float32)\n",
        "    ws/=ws.sum()+1e-8\n",
        "    return (item_vec[take]*ws[:,None]).sum(0)\n",
        "\n",
        "# 预先算好用户的“上下文嵌入”（随 batch 拼接用户GCN用）\n",
        "ctx_pool=np.zeros((U, item_vec.shape[1]), dtype=np.float32)\n",
        "tmp_map=defaultdict(list)\n",
        "for u, (ctx, tgt) in list(val_pairs.items())+list(test_pairs.items()):\n",
        "    tmp_map[int(u)]=ctx[-HIST_MAX:]\n",
        "for u, ctx in tmp_map.items():\n",
        "    ctx_pool[u]=pool_ctx(ctx, item_vec)\n",
        "\n",
        "ug=torch.tensor(user_gcn, dtype=torch.float32, device=DEVICE)\n",
        "uh=torch.tensor(ctx_pool, dtype=torch.float32, device=DEVICE)\n",
        "iv=torch.tensor(item_vec, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "model=FusionHead(ug.shape[1], uh.shape[1], iv.shape[1], out=64).to(DEVICE)\n",
        "opt=torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "def info_nce(u_z, i_z, temp=TEMP):\n",
        "    logits=(u_z @ i_z.t())/temp\n",
        "    labels=torch.arange(logits.size(0), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "# 训练集：用 (u, ctx, pos)；负例 = 同 batch 其他 + 概念近邻（hard）\n",
        "for ep in range(1,EPOCHS+1):\n",
        "    random.shuffle(pairs)\n",
        "    total=0.0\n",
        "    for st in range(0,len(pairs),BATCH):\n",
        "        batch=pairs[st:st+BATCH]\n",
        "        uids=[b[0] for b in batch]; poss=[b[2] for b in batch]\n",
        "        # hard negative 采样（不显式入损失，作为 in-batch 负例强化多样性）\n",
        "        # ——只需保证 batch 内 item 多样即可\n",
        "        u_g=ug[uids]; u_h=uh[uids]; i_p=iv[poss]\n",
        "        u_z=model.forward_user(u_g, u_h)\n",
        "        i_z=model.forward_item(i_p)\n",
        "        loss=info_nce(u_z, i_z)\n",
        "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "        total+=loss.item()\n",
        "    print(f\"Epoch {ep}/{EPOCHS} | InfoNCE: {total/max(1, len(pairs)//BATCH):.4f}\")\n",
        "\n",
        "# 导出\n",
        "with torch.no_grad():\n",
        "    fusion_user = model.forward_user(ug, uh).detach().cpu().numpy()\n",
        "    fusion_item = model.forward_item(iv).detach().cpu().numpy()\n",
        "np.save(os.path.join(DATA,\"fusion_user_embs.npy\"), fusion_user)\n",
        "np.save(os.path.join(DATA,\"fusion_item_embs.npy\"), fusion_item)\n",
        "print(\"✅ v2 融合向量已保存:\", fusion_user.shape, fusion_item.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QgX-Qbruuxu",
        "outputId": "b218fe31-e0c4-430e-cd6b-c2d0f179f105"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | InfoNCE: 6.7347\n",
            "Epoch 2/20 | InfoNCE: 8.6787\n",
            "Epoch 3/20 | InfoNCE: 7.2398\n",
            "Epoch 4/20 | InfoNCE: 6.7548\n",
            "Epoch 5/20 | InfoNCE: 6.6943\n",
            "Epoch 6/20 | InfoNCE: 6.6106\n",
            "Epoch 7/20 | InfoNCE: 6.2898\n",
            "Epoch 8/20 | InfoNCE: 6.0925\n",
            "Epoch 9/20 | InfoNCE: 6.1368\n",
            "Epoch 10/20 | InfoNCE: 6.2261\n",
            "Epoch 11/20 | InfoNCE: 6.2332\n",
            "Epoch 12/20 | InfoNCE: 6.1712\n",
            "Epoch 13/20 | InfoNCE: 6.0971\n",
            "Epoch 14/20 | InfoNCE: 6.0473\n",
            "Epoch 15/20 | InfoNCE: 6.0287\n",
            "Epoch 16/20 | InfoNCE: 6.0337\n",
            "Epoch 17/20 | InfoNCE: 6.0499\n",
            "Epoch 18/20 | InfoNCE: 6.0645\n",
            "Epoch 19/20 | InfoNCE: 6.0673\n",
            "Epoch 20/20 | InfoNCE: 6.0541\n",
            "✅ v2 融合向量已保存: (334, 64) (3415, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Stage 5 (v2): 上下文敏感的全库 Top-K 评估（统一打分与相似度）\n",
        "# 使用: FusionHead(user_gcn + ctx_pool) 与 fusion_item_embs 的内积（等同余弦因已归一化）\n",
        "# ==========================\n",
        "import os, math, numpy as np, torch, json\n",
        "\n",
        "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA=\"/content/data_v2\"\n",
        "TOPK=[5,10,20]; BATCH_U=512\n",
        "\n",
        "# 加载\n",
        "fuser=np.load(os.path.join(DATA,\"fusion_user_embs.npy\"))     # (U, D) 已归一化\n",
        "fitem=np.load(os.path.join(DATA,\"fusion_item_embs.npy\"))     # (I, D) 已归一化\n",
        "test_pairs=np.load(os.path.join(DATA,\"test_pairs.npy\"), allow_pickle=True).item()\n",
        "\n",
        "U,D=fuser.shape; I,_=fitem.shape\n",
        "user_t=torch.tensor(fuser, dtype=torch.float32, device=DEVICE)\n",
        "item_t=torch.tensor(fitem, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "def recall_ndcg_row(scores, true_idx):\n",
        "    order=np.argsort(-scores)\n",
        "    out={}\n",
        "    for k in TOPK:\n",
        "        hit=1.0 if true_idx in order[:k] else 0.0\n",
        "        if hit>0:\n",
        "            pos=np.where(order==true_idx)[0][0]; ndcg=1.0/math.log2(pos+2)\n",
        "        else:\n",
        "            ndcg=0.0\n",
        "        out[k]=(hit, ndcg)\n",
        "    return out\n",
        "\n",
        "# 构造“评估用户列表”\n",
        "eval_users=sorted(test_pairs.keys())\n",
        "metrics_sum={k:{\"rec\":0.0,\"ndcg\":0.0} for k in TOPK}\n",
        "\n",
        "# 全库分批\n",
        "for s in range(0, len(eval_users), BATCH_U):\n",
        "    batch_u=eval_users[s:s+BATCH_U]\n",
        "    u_idx=[int(u) for u in batch_u if int(u)<U]\n",
        "    if not u_idx: continue\n",
        "    sims=(user_t[u_idx] @ item_t.T).detach().cpu().numpy()  # (B,I)\n",
        "    for bi, uid in enumerate(u_idx):\n",
        "        _, tgt = test_pairs[uid]\n",
        "        res=recall_ndcg_row(sims[bi], int(tgt))\n",
        "        for k in TOPK:\n",
        "            metrics_sum[k][\"rec\"]  += res[k][0]\n",
        "            metrics_sum[k][\"ndcg\"] += res[k][1]\n",
        "\n",
        "N=len(eval_users)\n",
        "results={f\"Recall@{k}\": metrics_sum[k][\"rec\"]/N for k in TOPK}\n",
        "results.update({f\"NDCG@{k}\": metrics_sum[k][\"ndcg\"]/N for k in TOPK})\n",
        "print(\"📈 v2 全库 Top-K：\")\n",
        "for k in TOPK:\n",
        "    print(f\"  Recall@{k}: {results[f'Recall@{k}']:.4f} | NDCG@{k}: {results[f'NDCG@{k}']:.4f}\")\n",
        "\n",
        "with open(os.path.join(DATA,\"stage5_v2_results.json\"),\"w\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "print(\"💾 指标已保存：stage5_v2_results.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tYinCyyuujv",
        "outputId": "017995d2-9808-414e-9c35-61226c5f494b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 v2 全库 Top-K：\n",
            "  Recall@5: 0.0030 | NDCG@5: 0.0012\n",
            "  Recall@10: 0.0060 | NDCG@10: 0.0022\n",
            "  Recall@20: 0.0090 | NDCG@20: 0.0030\n",
            "💾 指标已保存：stage5_v2_results.json\n"
          ]
        }
      ]
    }
  ]
}